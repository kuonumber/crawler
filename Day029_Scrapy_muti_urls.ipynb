{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day029_Scrapy_muti_urls.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNQdUlw0y8Tfd4vG+o2d/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuonumber/crawler/blob/master/Day029_Scrapy_muti_urls.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru7BFAo-ImFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CxAAh_uLusW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests, zipfile, io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfC8sdFKUkpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install Scrapy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxHykIVZJFKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_file_url = 'https://pycrawler-fileentity.cupoy.com/marathon/homework/data/1586231311460/Day029_Scrapy_PTT.zip?t=1586231316018'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFXiBzgEJt7P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "bfa5e35a-e024-454c-80fd-29d7ea15e160"
      },
      "source": [
        "! wget $zip_file_url"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-21 02:51:24--  https://pycrawler-fileentity.cupoy.com/marathon/homework/data/1586231311460/Day029_Scrapy_PTT.zip?t=1586231316018\n",
            "Resolving pycrawler-fileentity.cupoy.com (pycrawler-fileentity.cupoy.com)... 54.192.86.99, 54.192.86.92, 54.192.86.59, ...\n",
            "Connecting to pycrawler-fileentity.cupoy.com (pycrawler-fileentity.cupoy.com)|54.192.86.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23912 (23K) [application/octet-stream]\n",
            "Saving to: ‘Day029_Scrapy_PTT.zip?t=1586231316018’\n",
            "\n",
            "Day029_Scrapy_PTT.z 100%[===================>]  23.35K   146KB/s    in 0.2s    \n",
            "\n",
            "2020-09-21 02:51:25 (146 KB/s) - ‘Day029_Scrapy_PTT.zip?t=1586231316018’ saved [23912/23912]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B52Igz1PJuA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ec03fd7-cf9a-4d42-e3c5-756e686e78e8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Day029_Scrapy_PTT.zip?t=1586231316018'   __MACOSX   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugZzNSvOKBUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d7b2825-d840-4041-b3d1-4f863f2c6195"
      },
      "source": [
        "zipfile.is_zipfile('Day029_Scrapy_PTT.zip?t=1586231316018')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd8mak84KWVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip = zipfile.ZipFile('Day029_Scrapy_PTT.zip?t=1586231316018')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79mzlqB5Ki_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "e9bc8faf-0319-4cc4-b5b9-f77e271c9750"
      },
      "source": [
        "zip.namelist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Day029_Scrapy_PTT/',\n",
              " '__MACOSX/._Day029_Scrapy_PTT',\n",
              " 'Day029_Scrapy_PTT/myproject/',\n",
              " '__MACOSX/Day029_Scrapy_PTT/._myproject',\n",
              " 'Day029_Scrapy_PTT/Pipfile',\n",
              " '__MACOSX/Day029_Scrapy_PTT/._Pipfile',\n",
              " 'Day029_Scrapy_PTT/Pipfile.lock',\n",
              " '__MACOSX/Day029_Scrapy_PTT/._Pipfile.lock',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/._myproject',\n",
              " 'Day029_Scrapy_PTT/myproject/scrapy.cfg',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/._scrapy.cfg',\n",
              " 'Day029_Scrapy_PTT/myproject/main.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/._main.py',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/spiders/',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/._spiders',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/__init__.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/.___init__.py',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/middlewares.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/._middlewares.py',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/settings.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/._settings.py',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/items.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/._items.py',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/pipelines.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/._pipelines.py',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/spiders/PTTCrawler.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/spiders/._PTTCrawler.py',\n",
              " 'Day029_Scrapy_PTT/myproject/myproject/spiders/__init__.py',\n",
              " '__MACOSX/Day029_Scrapy_PTT/myproject/myproject/spiders/.___init__.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CWb2AH1I6pb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-imEqNZpKyT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "47c4ef3b-3597-40ff-f4b0-9330d0ab3048"
      },
      "source": [
        "!ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Day029_Scrapy_PTT\t\t\t  __MACOSX\n",
            "'Day029_Scrapy_PTT.zip?t=1586231316018'   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnQGXi1QMtk2",
        "colab_type": "text"
      },
      "source": [
        "# my own code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWrtYXurMeW6",
        "colab_type": "text"
      },
      "source": [
        "## use shell in cell\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdJvqnvTctRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxG13nuQhNt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from kora import console\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6E1ZbEWkQPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3e9de88-c5ad-4679-9044-e70f7fadccb0"
      },
      "source": [
        "console.start( ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Console URL: https://teleconsole.com/s/de4128a7d243963678fd5cbb22d666e665b1d82b\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwmzJEvJdEWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import JSON\n",
        "from google.colab import output\n",
        "from subprocess import getoutput\n",
        "import os\n",
        "\n",
        "def shell(command):\n",
        "  if command.startswith('cd'):\n",
        "    path = command.strip().split(maxsplit=1)[1]\n",
        "    os.chdir(path)\n",
        "    return JSON([''])\n",
        "  return JSON([getoutput(command)])\n",
        "output.register_callback('shell', shell)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r92Fbb0wdK3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Colab Shell\n",
        "%%html\n",
        "<div id=term_demo></div>\n",
        "<script src=\"https://code.jquery.com/jquery-latest.js\"></script>\n",
        "<script src=\"https://cdn.jsdelivr.net/npm/jquery.terminal/js/jquery.terminal.min.js\"></script>\n",
        "<link href=\"https://cdn.jsdelivr.net/npm/jquery.terminal/css/jquery.terminal.min.css\" rel=\"stylesheet\"/>\n",
        "<script>\n",
        "  $('#term_demo').terminal(async function(command) {\n",
        "      if (command !== '') {\n",
        "          try {\n",
        "              let res = await google.colab.kernel.invokeFunction('shell', [command])\n",
        "              let out = res.data['application/json'][0]\n",
        "              this.echo(new String(out))\n",
        "          } catch(e) {\n",
        "              this.error(new String(e));\n",
        "          }\n",
        "      } else {\n",
        "          this.echo('');\n",
        "      }\n",
        "  }, {\n",
        "      greetings: 'Welcome to Colab Shell',\n",
        "      name: 'colab_demo',\n",
        "      height: 250,\n",
        "      prompt: 'colab > '\n",
        "  });"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDabU1AOuQyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bash "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQk5A1OMb7I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " %%shell\n",
        "scrapy shell \"http://quotes.toscrape.com/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5MGztK-Mk4w",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6RzQ0opMl72",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EisJgDZ1qCqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1ivIl-LqCnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://www.inside.com.tw/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1GpAgbvqChf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = requests.get(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX6yN6VZqCY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup = BeautifulSoup(res.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8JCcZPHqfBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2b09bc3c-f4e7-4fe0-aa86-c8aa931be141"
      },
      "source": [
        "soup.select('.js-auto_break_title')[0]['href']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://www.inside.com.tw/article/20956-2020-apple-event-watch-6-SE'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k-ltgrJYdAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[u['href'] for u in soup.select('.js-auto_break_title')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_RHCJpKaTVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = requests.get([u['href'] for u in soup.select('.js-auto_break_title')][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R-ONDfSaTZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup = BeautifulSoup(res.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTnk1PwMaUOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "eb3737fd-79ff-4451-cf28-8d9737b0bc26"
      },
      "source": [
        "soup.select('.post_header_title')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<h1 class=\"post_header_title js-auto_break_title\">蘋果發表會 Apple Watch Series 6、Watch SE 與 Apple One 登場</h1>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXQWy_Lf0ahO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "d2669abf-3763-451d-b4db-e56b2b1e3c8d"
      },
      "source": [
        "[pp.text for line in soup.select('#article_content') for pp in line.select('p')]\n",
        "# .select('p')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['蘋果秋季發表會於今日美國加州時間上午十點，台灣時間凌晨一點登場，一如往常地由蘋果大家長-執行長庫克（Tim Cook）於 Apple Park 為年度的盛會揭開序幕，名為「Time Flies」的主題，時光飛逝自疫情爆發以來度過半年的時間，今年疫情影響數位科技與網路更是更顯重要，像是遠距工作、遠距學習大需求，庫克以破題法告訴大家今天聚焦在 Apple Watch 以及 iPad。',\n",
              " '過去在 Apple Watch 上救命的案例不少，也幫助高血壓患者、視障者等在生活上更便利，而因應疫情，在今年市場更加關注醫療話題，本次也聚焦在健康功能。',\n",
              " '首先，如預期的推出 Apple Watch Series 6，除了藉由 watchOS 7 實現睡眠追蹤，也新增傳說中的血氧功能，可透過動作和心率測量最大攝氧量的數值區間，這是過去專業診所上提供的功能，如今搭配 watchOS 7 也能實現進行預測，並在數值異常時發出提醒。',\n",
              " '在外觀部分，藍色金屬表面、金色不鏽鋼外觀、還有石墨色的黑灰不鏽鋼金屬錶殼，更首次帶來紅色的錶殼，同時也推出一款新的單圈錶帶，彈性、防水更簡約，還有編織的錶帶、新色 Nike 錶帶。另外，與藝術家合作推出更多錶面，也能使用自己的 memoji 更加趣味。',\n",
              " '根據介紹，透過感應器計算用戶血液顏色得出血液的含氧量，15 秒完成一次測量。同時在後台上會定期產出量測報告並儲存在健康 App 上，在今年疫情之際，血氧和動脈測氧器經常被提起，血氧飽和度能知道目前氧氣輸送的情形，和整體的心肺健康，而現在也能利用血氧 App 助力於相關的醫學研究上，Apple 已於許多醫療專家與機構合作。',\n",
              " '採用第六代晶片 S6，雙核處理器由基於 iPhone 11 的 A13 打造，並根據 Apple Watch 需求做優化，與前代相比，速度快了 20%，也強調 Watch 使用 100% 循環利用的材質打造，Apple Watch Series 6 GPS 版售價 399 \\xa0美元。',\n",
              " '另外，在戶外更多人性化的功能，比如戶外亮度自動增加 2.5倍，可以監測海拔高度。當然也可以充分利用 Watch \\xa0上的 App 實現更多功能，像是追蹤太陽位置。',\n",
              " '再來是家長安心孩子更小心的家人共享功能，新增「家庭配對」（Family Setup）即便孩子、年邁長輩沒有手機，仍然可以啟動定位通知，遠端監控勿擾模式、限制互動，了解孩子的學習狀態，設置需要 Apple Watch 4 以上 LTE 版本，支援十二個國家電信商，台灣中華電信也出現在發表會的投影片中。',\n",
              " '接著，如預期中推出平價版的 Apple Watch SE ，結合 Apple Watch Series 6 設計，包含健身等核心功能的平價版智慧手錶，基本的加速感應器、陀螺儀、指南針與高度計都包含，也有最新的運動感應器，因此也支援跌倒監測，而 Watch SE 採用 S5 晶片，運作效能比 Series 3 快兩倍，售價 279 美元起，支援分期付款每月 12 \\xa0美元。',\n",
              " 'Apple Watch Series 6 台幣 12900 元起與 Apple Watch SE 台幣 8900 元起，9/17 開放預購，9/23 開始陸續出貨。',\n",
              " '另外，蘋果推出健身訂閱服務 Fitness+，更便利在運動訓練上，能在大螢幕上顯示當下運動數據，有 10 種不同的鍛煉類型，包含騎自行車、跑步機、瑜伽、核心訓練、划船等。如果速度太快或太慢，Apple Watch 也會提醒，並提供更精確的心率、每分鐘步數等運動數值，Fitness＋ 將在澳洲、加拿大、愛爾蘭、紐西蘭、英國和美國年底前上市，每月 9.99 美元，一年訂閱優惠 79 .99 美元。',\n",
              " '接著也一同宣布傳聞中的訂閱套餐服務，全新的訂閱服務「Apple One」，讓用戶可以一次訂購多種 Apple 軟體服務，包含 Apple Music、TV+、Arcade、Fitness+，三種方案包含個人、家庭與進階版，分別是14.95 \\xa0美元、19.95 \\xa0美元、29.95 \\xa0美元。',\n",
              " '根據 Apple 說法，目前標準每月定價，個人方案每月可節省超過 6 美元，而家庭計劃每月可節省 8 美元。Premier 計劃每月可節省超過 25 美元。',\n",
              " '3:18 更新 Apple One 方案台灣售價：',\n",
              " '核稿編輯：李柏鋒',\n",
              " '延伸閱讀：',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLimWAIy0apS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNzm0EUEaUSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb0cc823-f7b8-41c8-c788-60adfa34a0d3"
      },
      "source": [
        "! pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk4eOV7tii3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "a8f4144d-1b00-4ea2-b220-389517f62384"
      },
      "source": [
        "!scrapy startproject myproject_inside"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New Scrapy project 'myproject_inside', using template directory '/usr/local/lib/python3.6/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/myproject_inside\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd myproject_inside\n",
            "    scrapy genspider example example.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZURdMYlOi5IP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c091c33b-eb18-4a04-ab34-81f0866109eb"
      },
      "source": [
        "!scrapy genspider InsideCrawler inside.com.tw\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created spider 'InsideCrawler' using template 'basic' \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66amVMI0kJSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from PTTCrawler import PttcrawlerSpider"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTEoU7Lnktqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PttcrawlerSpider.start_requests(self)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCidItvuqfJE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "a15556e9-1c69-49f8-b684-47a157609a73"
      },
      "source": [
        "# !scrapy crawl InsideCrawler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scrapy 2.3.0 - no active project\n",
            "\n",
            "Unknown command: crawl\n",
            "\n",
            "Use \"scrapy\" to see available commands\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkKvKBelbO6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy runspider InsideCrawler.py -s USER_AGENT='Mozilla/5.0'\n",
        "\n",
        "''' 403 代表有防爬蟲，\n",
        "1. 可以再command line 加入-s參數 \n",
        "2. 可以加入此段程式碼\n",
        "def start_requests(self):\n",
        "    yield scrapy.Request('https://www.inside.com.tw/',\n",
        "          headers={'User-Agent': \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\"})\n",
        "3. 調整 setting 中的 user-agent 項目    \n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0VdBTUL411w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f29b50a6-3991-468e-a100-9ec8db369c5f"
      },
      "source": [
        "!scrapy runspider InsideCrawler.py -s USER_AGENT='Mozilla/5.0' \n",
        "# -o inside.json -t json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-17 09:51:39 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: scrapybot)\n",
            "2020-09-17 09:51:39 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Jul 17 2020, 12:50:27) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.1, Platform Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-09-17 09:51:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-09-17 09:51:39 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'SPIDER_LOADER_WARN_ONLY': True, 'USER_AGENT': 'Mozilla/5.0'}\n",
            "2020-09-17 09:51:39 [scrapy.extensions.telnet] INFO: Telnet Password: e6152d710a4ff2b4\n",
            "2020-09-17 09:51:39 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-09-17 09:51:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-09-17 09:51:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-09-17 09:51:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2020-09-17 09:51:39 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-09-17 09:51:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-09-17 09:51:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2020-09-17 09:51:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.inside.com.tw/> (referer: None)\n",
            "2020-09-17 09:51:39 [scrapy.core.engine] DEBUG: Crawled (400) <GET https://www.inside.com.tw?page=2> (referer: https://www.inside.com.tw/)\n",
            "2020-09-17 09:51:39 [scrapy.core.engine] DEBUG: Crawled (400) <GET https://www.inside.com.tw?page=3> (referer: https://www.inside.com.tw/)\n",
            "2020-09-17 09:51:39 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2020-09-17 09:51:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 1933,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 19666,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/400': 2,\n",
            " 'elapsed_time_seconds': 0.556291,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2020, 9, 17, 9, 51, 39, 983850),\n",
            " 'log_count/DEBUG': 3,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 144805888,\n",
            " 'memusage/startup': 144805888,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 3,\n",
            " 'scheduler/dequeued': 3,\n",
            " 'scheduler/dequeued/memory': 3,\n",
            " 'scheduler/enqueued': 3,\n",
            " 'scheduler/enqueued/memory': 3,\n",
            " 'start_time': datetime.datetime(2020, 9, 17, 9, 51, 39, 427559)}\n",
            "2020-09-17 09:51:39 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpjRypZ05RNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "/content/InsideCrawler.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydKsSeE8tOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# !scrapy runspider InsideCrawler.py \n",
        "\n",
        "# Following hyperlink and “Filtered offsite request” - https://stackoverflow.com/questions/17862474/following-hyperlink-and-filtered-offsite-request\n",
        "# allowed_domains , 不應該包含\"http://www\", \"https://www\" 和 結尾有 \"/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-LjePF1cjVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !scrapy runspider InsideCrawler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ttJvPh6sS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "6d45f748-e74d-4302-c9cb-670648e56cd4"
      },
      "source": [
        "%cat InsideCrawler.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "import scrapy\n",
            "from bs4 import BeautifulSoup\n",
            "from myproject_inside.myproject_inside.items import MyprojectInsideItem\n",
            "from scrapy.spiders import CrawlSpider, Rule\n",
            "from scrapy.linkextractors import  LinkExtractor\n",
            "\n",
            "class InsidecrawlerSpider(CrawlSpider):\n",
            "\n",
            "# class InsidecrawlerSpider(scrapy.Spider):\n",
            "    name = 'InsideCrawler'\n",
            "    allowed_domains = ['inside.com.tw']\n",
            "    start_urls = ['http://www.inside.com.tw/']\n",
            "    rules = [\n",
            "        Rule(LinkExtractor(allow=('/?page=[1-3]$')),\n",
            "         callback='parse_list', follow=True)\n",
            "    ]\n",
            "\n",
            "    def parse_list(self, response):\n",
            "        soup = BeautifulSoup(response.body, 'lxml')\n",
            "        for t in soup.select('.js-auto_break_title'):\n",
            "            # print(t.text)\n",
            "            yield scrapy.Request(t['href'], self.parse_detail)\n",
            "            \n",
            "    def start_requests(self):\n",
            "        yield scrapy.Request('https://www.inside.com.tw/',\n",
            "                      headers={'User-Agent': \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36\"})\n",
            "    \n",
            "    def parse_detail(self, response):\n",
            "        print(\"SCRAP EVERY LINK\")\n",
            "        soup = BeautifulSoup(response.body, 'lxml')\n",
            "        print(soup.select('.post_header_title')[0].text)\n",
            "        inside_item = MyprojectInsideItem()\n",
            "        inside_item['title'] = soup.select('.post_header_title')[0].text\n",
            "        inside_item['content'] =  [pp.text for line in soup.select('#article_content') for pp in line.select('p')]\n",
            "        return inside_item\n",
            "        # print(soup)"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B8HLDlf-HEd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "8006719c-a3d5-4ae4-b0ce-497c62900c97"
      },
      "source": [
        "%cat /content/myproject_inside/myproject_inside/items.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Define here the models for your scraped items\n",
            "#\n",
            "# See documentation in:\n",
            "# https://docs.scrapy.org/en/latest/topics/items.html\n",
            "\n",
            "import scrapy\n",
            "\n",
            "\n",
            "class MyprojectInsideItem(scrapy.Item):\n",
            "    # define the fields for your item here like:\n",
            "    title = scrapy.Field()\n",
            "    content = scrapy.Field()\n",
            "    # time = scrapy.Field()\n",
            "    # pass\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnUGfytHjGZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b32cchT9jHrU",
        "colab_type": "text"
      },
      "source": [
        "# pipline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEleS9LZp2o5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d628f64e-6173-4d72-85d0-e8b8aa8bebf9"
      },
      "source": [
        "%cat /content/myproject_inside/myproject_inside/pipelines.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Define your item pipelines here\n",
            "#\n",
            "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
            "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
            "import sqlite3\n",
            "import os\n",
            "import json\n",
            "\n",
            "from pathlib import Path\n",
            "from datetime import datetime\n",
            "\n",
            "# useful for handling different item types with a single interface\n",
            "from itemadapter import ItemAdapter\n",
            "\n",
            "class MyprojectInsidePipeline(object):\n",
            "    def open_spider(self, spider):\n",
            "        self.conn = sqlite3.connect('inside.sqlite')\n",
            "        self.cur = self.conn.cursor()\n",
            "        self.cur.execute('create table if not exists inside(title varchar(100), content text)')\n",
            "\n",
            "         self.start_crawl_datetime = datetime.now().strftime('%Y%m%dT%H:%M:%S')\n",
            "\n",
            "        # 在開始爬蟲的時候建立暫時的 JSON 檔案\n",
            "        # 避免有多筆爬蟲結果的時候，途中發生錯誤導致程式停止會遺失所有檔案\n",
            "        self.dir_path = Path(__file__).resolve().parents[1] / 'crawled_data'\n",
            "        self.runtime_file_path = str(self.dir_path / '.tmp.json.swp')\n",
            "        if not self.dir_path.exists():\n",
            "            self.dir_path.mkdir(parents=True)\n",
            "        spider.log('Create temp file for store JSON - {}'.format(self.runtime_file_path))\n",
            "\n",
            "        # 設計 JSON 存的格式為\n",
            "        # [\n",
            "        #  {...}, # 一筆爬蟲結果\n",
            "        #  {...}, ...\n",
            "        # ]\n",
            "        self.runtime_file = open(self.runtime_file_path, 'w+', encoding='utf8')\n",
            "        self.runtime_file.write('[\\n')\n",
            "        self._first_item = True\n",
            "    \n",
            "    def close_spider(self, spider):\n",
            "        self.conn.commit()\n",
            "        self.conn.close()\n",
            "\n",
            "        self.end_crawl_datetime = datetime.now().strftime('%Y%m%dT%H:%M:%S')\n",
            "\n",
            "        # 儲存 JSON 格式\n",
            "        self.runtime_file.write('\\n]')\n",
            "        self.runtime_file.close()\n",
            "        \n",
            "        # 將暫存檔改為以日期為檔名的格式\n",
            "        self.store_file_path = self.dir_path / '{}-{}.json'.format(self.start_crawl_datetime,\n",
            "                                                                   self.end_crawl_datetime)\n",
            "        self.store_file_path = str(self.store_file_path)\n",
            "        os.rename(self.runtime_file_path, self.store_file_path)\n",
            "        spider.log('Save result at {}'.format(self.store_file_path))\n",
            "\n",
            "    \n",
            "    def process_item(self, item, spider):\n",
            "        col = ','.join(item.keys())\n",
            "        placeholders = ','.join(len(item) * '?')\n",
            "        sql = 'insert into inside({}) values({})'\n",
            "        self.cur.execute(sql.format(col, placeholders), tuple(item.values()))\n",
            "        return item\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU_O3r9rjGj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!scrapy runspider InsideCrawler.py -s USER_AGENT='Mozilla/5.0' \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwicPxDRm32i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE3XJu1B_vnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import  LinkExtractor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koxs472jdk5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Rule(  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRjh2-fFYObK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LinkExtractor(allow=('https://www.inside.com.tw/?page=[1-3]$'), callback=parse_list, follow=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE0v7oNRyb-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahxb-UkwydJu",
        "colab_type": "text"
      },
      "source": [
        "# scrapy API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTzzPFp8ygVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "from scrapy.utils.project import get_project_settings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpHsHG3L0npd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_project_settings()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBqpr51ygwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff19b43d-9dff-4152-c9e7-a1dd968a51d1"
      },
      "source": [
        "%cd /content/Day028_Scrapy_API/myproject/myproject/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Day028_Scrapy_API/myproject/myproject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ucO5HT5SYlV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9007634-491c-4b61-b0a7-871524d3775d"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Day028_Scrapy_API/myproject/myproject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsekAiIZSZwZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "49783d72-e7b5-45a1-af07-50bb0c850a87"
      },
      "source": [
        "!scrapy crawl PTTCrawler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/scrapy\", line 8, in <module>\n",
            "    sys.exit(execute())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scrapy/cmdline.py\", line 114, in execute\n",
            "    settings = get_project_settings()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scrapy/utils/project.py\", line 69, in get_project_settings\n",
            "    settings.setmodule(settings_module_path, priority='project')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scrapy/settings/__init__.py\", line 286, in setmodule\n",
            "    module = import_module(module)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 941, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
            "  File \"<frozen importlib._bootstrap>\", line 994, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
            "ModuleNotFoundError: No module named 'myproject'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1RK_WJgSdMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 747
        },
        "outputId": "76d14d72-03e0-44c0-cd5b-bbd4ea3ef00f"
      },
      "source": [
        "target_urls = [\n",
        "         'https://www.ptt.cc/bbs/Gossiping/M.1600501556.A.6D3.html',\n",
        "         'https://www.ptt.cc/bbs/Gossiping/M.1600501600.A.E3C.html',\n",
        "    ]\n",
        "# process = CrawlerProcess(get_project_settings( ))\n",
        "process = CrawlerProcess(get_project_settings( ))\n",
        "\n",
        "process.crawl('PTTCrawler', start_urls=target_urls, filename='test.json')\n",
        "# process.start(stop_after_crawl=False)\n",
        "# process.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-19 08:06:23 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: myproject)\n",
            "2020-09-19 08:06:23 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Jul 17 2020, 12:50:27) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.1, Platform Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-09-19 08:06:23 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-09-19 08:06:23 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'myproject',\n",
            " 'NEWSPIDER_MODULE': 'myproject.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['myproject.spiders']}\n",
            "2020-09-19 08:06:23 [scrapy.extensions.telnet] INFO: Telnet Password: 49830b821cfc5802\n",
            "2020-09-19 08:06:23 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-09-19 08:06:23 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-09-19 08:06:23 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-09-19 08:06:23 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "['myproject.pipelines.JSONPipeline']\n",
            "2020-09-19 08:06:23 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-09-19 08:06:23 [PTTCrawler] DEBUG: Create temp file for store JSON - /content/Day028_Scrapy_API/myproject/crawled_data/.tmp.json.swp\n",
            "2020-09-19 08:06:23 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-09-19 08:06:23 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6032\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Deferred at 0x7fe1bc47e240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_LtJhKwZEsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "outputId": "a0ee3122-5fc0-43b8-f08f-eadac47dd12a"
      },
      "source": [
        "process.crawl( )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-19 08:04:09 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'myproject',\n",
            " 'NEWSPIDER_MODULE': 'myproject.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['myproject.spiders']}\n",
            "2020-09-19 08:04:09 [scrapy.extensions.telnet] INFO: Telnet Password: 055d8f5c07dbc00c\n",
            "2020-09-19 08:04:09 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-09-19 08:04:09 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-09-19 08:04:09 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-09-19 08:04:09 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "['myproject.pipelines.JSONPipeline']\n",
            "2020-09-19 08:04:09 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-09-19 08:04:09 [PTTCrawler] DEBUG: Create temp file for store JSON - /content/Day028_Scrapy_API/myproject/crawled_data/.tmp.json.swp\n",
            "2020-09-19 08:04:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-09-19 08:04:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6030\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Deferred at 0x7fe1bc516dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj7jh0AxX87A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "3c026e6f-c5c7-414c-c8ae-ac9c614f9937"
      },
      "source": [
        "process.stop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-19 07:59:15 [scrapy.core.engine] INFO: Closing spider (shutdown)\n",
            "2020-09-19 07:59:15 [PTTCrawler] DEBUG: Save result at /content/Day028_Scrapy_API/myproject/crawled_data/test.json\n",
            "2020-09-19 07:59:15 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'elapsed_time_seconds': 14.4094,\n",
            " 'finish_reason': 'shutdown',\n",
            " 'finish_time': datetime.datetime(2020, 9, 19, 7, 59, 15, 79798),\n",
            " 'log_count/DEBUG': 2,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 190656512,\n",
            " 'memusage/startup': 190656512,\n",
            " 'start_time': datetime.datetime(2020, 9, 19, 7, 59, 0, 670398)}\n",
            "2020-09-19 07:59:15 [scrapy.core.engine] INFO: Spider closed (shutdown)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DeferredList at 0x7fe1bc558cf8 current result: [(True, None)]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVGOnc2DTa3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHoyWC3enhf-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z4fO7-TqPIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c8ef81a-84da-442e-cfe1-7b9d821a8859"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wrn9YOgjnhxz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1894ac7f-20e0-4bde-8df4-ebf12bf57a5c"
      },
      "source": [
        "%cd /content/Day029_Scrapy_PTT/myproject/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Day029_Scrapy_PTT/myproject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhyxslmjnwpu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f2e0f54-d0de-4cd0-b144-25439a42d09f"
      },
      "source": [
        "!scrapy crawl PTTCrawler -a board=Stock"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-09-21 03:20:28 [scrapy.utils.log] INFO: Scrapy 2.3.0 started (bot: myproject)\n",
            "2020-09-21 03:20:28 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Jul 17 2020, 12:50:27) - [GCC 8.4.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020), cryptography 3.1, Platform Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2020-09-21 03:20:28 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2020-09-21 03:20:28 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'myproject',\n",
            " 'NEWSPIDER_MODULE': 'myproject.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['myproject.spiders']}\n",
            "2020-09-21 03:20:28 [scrapy.extensions.telnet] INFO: Telnet Password: d1c6bd8309eed5c7\n",
            "2020-09-21 03:20:28 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2020-09-21 03:20:28 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2020-09-21 03:20:28 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2020-09-21 03:20:28 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "['myproject.pipelines.JSONPipeline']\n",
            "2020-09-21 03:20:28 [scrapy.core.engine] INFO: Spider opened\n",
            "2020-09-21 03:20:28 [PTTCrawler] DEBUG: Create temp file for store JSON - /content/Day029_Scrapy_PTT/myproject/crawled_data/.tmp.json.swp\n",
            "2020-09-21 03:20:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-09-21 03:20:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2020-09-21 03:20:43 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://www.ptt.cc/robots.txt> (referer: None)\n",
            "2020-09-21 03:21:14 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.ptt.cc/bbs/Stock/index.html> (failed 1 times): 522 Unknown Status\n",
            "2020-09-21 03:21:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.ptt.cc/bbs/Stock/index.html> (referer: None)\n",
            "/content/Day029_Scrapy_PTT/myproject/myproject/spiders/PTTCrawler.py:24: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 24 of the file /content/Day029_Scrapy_PTT/myproject/myproject/spiders/PTTCrawler.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  soup = BeautifulSoup(response.text)\n",
            "2020-09-21 03:21:14 [PTTCrawler] DEBUG: Parse article Re: [請益] 手中300萬但不知如何踏出第一步\n",
            "2020-09-21 03:21:14 [PTTCrawler] DEBUG: Parse article  [標的] 2408 南亞科\n",
            "2020-09-21 03:21:14 [PTTCrawler] DEBUG: Parse article Re: [請益] 有多少人投資股票是按照 凱利公式 比例投\n",
            "2020-09-21 03:21:14 [PTTCrawler] DEBUG: Reach the last article\n",
            "2020-09-21 03:21:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.ptt.cc/bbs/Stock/M.1600393110.A.305.html> (failed 1 times): 522 Unknown Status\n",
            "2020-09-21 03:21:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.ptt.cc/bbs/Stock/M.1600392654.A.C1A.html> (failed 1 times): 522 Unknown Status\n",
            "2020-09-21 03:21:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.ptt.cc/bbs/Stock/M.1600389606.A.A21.html> (failed 1 times): 522 Unknown Status\n",
            "2020-09-21 03:21:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.ptt.cc/bbs/Stock/M.1600393110.A.305.html> (failed 2 times): 522 Unknown Status\n",
            "2020-09-21 03:21:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.ptt.cc/bbs/Stock/M.1600392654.A.C1A.html> (failed 2 times): 522 Unknown Status\n",
            "2020-09-21 03:21:15 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://www.ptt.cc/bbs/Stock/M.1600389606.A.A21.html> (failed 2 times): 522 Unknown Status\n",
            "2020-09-21 03:21:28 [scrapy.extensions.logstats] INFO: Crawled 2 pages (at 2 pages/min), scraped 0 items (at 0 items/min)\n",
            "2020-09-21 03:21:45 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.ptt.cc/bbs/Stock/M.1600393110.A.305.html> (failed 3 times): 522 Unknown Status\n",
            "2020-09-21 03:21:45 [scrapy.core.engine] DEBUG: Crawled (522) <GET https://www.ptt.cc/bbs/Stock/M.1600393110.A.305.html> (referer: https://www.ptt.cc/bbs/Stock/index.html)\n",
            "2020-09-21 03:21:45 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <522 https://www.ptt.cc/bbs/Stock/M.1600393110.A.305.html>: HTTP status code is not handled or not allowed\n",
            "2020-09-21 03:21:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.ptt.cc/bbs/Stock/M.1600392654.A.C1A.html> (failed 3 times): 522 Unknown Status\n",
            "2020-09-21 03:21:46 [scrapy.core.engine] DEBUG: Crawled (522) <GET https://www.ptt.cc/bbs/Stock/M.1600392654.A.C1A.html> (referer: https://www.ptt.cc/bbs/Stock/index.html)\n",
            "2020-09-21 03:21:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <522 https://www.ptt.cc/bbs/Stock/M.1600392654.A.C1A.html>: HTTP status code is not handled or not allowed\n",
            "2020-09-21 03:21:46 [scrapy.downloadermiddlewares.retry] ERROR: Gave up retrying <GET https://www.ptt.cc/bbs/Stock/M.1600389606.A.A21.html> (failed 3 times): 522 Unknown Status\n",
            "2020-09-21 03:21:46 [scrapy.core.engine] DEBUG: Crawled (522) <GET https://www.ptt.cc/bbs/Stock/M.1600389606.A.A21.html> (referer: https://www.ptt.cc/bbs/Stock/index.html)\n",
            "2020-09-21 03:21:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <522 https://www.ptt.cc/bbs/Stock/M.1600389606.A.A21.html>: HTTP status code is not handled or not allowed\n",
            "2020-09-21 03:21:46 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2020-09-21 03:21:46 [PTTCrawler] DEBUG: Save result at /content/Day029_Scrapy_PTT/myproject/crawled_data/Stock-20200921T03:21:46.json\n",
            "2020-09-21 03:21:46 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 5463,\n",
            " 'downloader/request_count': 12,\n",
            " 'downloader/request_method_count/GET': 12,\n",
            " 'downloader/response_bytes': 70624,\n",
            " 'downloader/response_count': 12,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'downloader/response_status_count/522': 10,\n",
            " 'elapsed_time_seconds': 78.198143,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2020, 9, 21, 3, 21, 46, 460390),\n",
            " 'httperror/response_ignored_count': 3,\n",
            " 'httperror/response_ignored_status_count/522': 3,\n",
            " 'log_count/DEBUG': 18,\n",
            " 'log_count/ERROR': 3,\n",
            " 'log_count/INFO': 14,\n",
            " 'memusage/max': 88563712,\n",
            " 'memusage/startup': 88563712,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 5,\n",
            " 'retry/count': 7,\n",
            " 'retry/max_reached': 3,\n",
            " 'retry/reason_count/522 Unknown Status': 7,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 11,\n",
            " 'scheduler/dequeued/memory': 11,\n",
            " 'scheduler/enqueued': 11,\n",
            " 'scheduler/enqueued/memory': 11,\n",
            " 'start_time': datetime.datetime(2020, 9, 21, 3, 20, 28, 262247)}\n",
            "2020-09-21 03:21:46 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8tNsYDDG949",
        "colab_type": "text"
      },
      "source": [
        "# understand scrapy comman line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daK5wI3goNd9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "074c91b8-8368-4d63-fb33-6a021dfad853"
      },
      "source": [
        "!scrapy crawl -h\n",
        "# 必須要在專案資料夾下面，才可以使用crawl指令 其他相關請查看連結\n",
        "# https://scrapy-gallaecio.readthedocs.io/en/latest/topics/commands.html#topics-commands-ref"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage\n",
            "=====\n",
            "  scrapy crawl [options] <spider>\n",
            "\n",
            "Run a spider\n",
            "\n",
            "Options\n",
            "=======\n",
            "--help, -h              show this help message and exit\n",
            "-a NAME=VALUE           set spider argument (may be repeated)\n",
            "--output=FILE, -o FILE  dump scraped items into FILE (use - for stdout)\n",
            "--output-format=FORMAT, -t FORMAT\n",
            "                        format to use for dumping items with -o\n",
            "\n",
            "Global Options\n",
            "--------------\n",
            "--logfile=FILE          log file. if omitted stderr will be used\n",
            "--loglevel=LEVEL, -L LEVEL\n",
            "                        log level (default: DEBUG)\n",
            "--nolog                 disable logging completely\n",
            "--profile=FILE          write python cProfile stats to FILE\n",
            "--pidfile=FILE          write process ID to FILE\n",
            "--set=NAME=VALUE, -s NAME=VALUE\n",
            "                        set/override setting (may be repeated)\n",
            "--pdb                   enable pdb on failure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Owo4V4ishQ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "0ed73a4a-1a31-4049-e621-4f30a6f678a1"
      },
      "source": [
        "!scrapy runspider -h"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Usage\n",
            "=====\n",
            "  scrapy runspider [options] <spider_file>\n",
            "\n",
            "Run the spider defined in the given file\n",
            "\n",
            "Options\n",
            "=======\n",
            "--help, -h              show this help message and exit\n",
            "-a NAME=VALUE           set spider argument (may be repeated)\n",
            "--output=FILE, -o FILE  dump scraped items into FILE (use - for stdout)\n",
            "--output-format=FORMAT, -t FORMAT\n",
            "                        format to use for dumping items with -o\n",
            "\n",
            "Global Options\n",
            "--------------\n",
            "--logfile=FILE          log file. if omitted stderr will be used\n",
            "--loglevel=LEVEL, -L LEVEL\n",
            "                        log level (default: DEBUG)\n",
            "--nolog                 disable logging completely\n",
            "--profile=FILE          write python cProfile stats to FILE\n",
            "--pidfile=FILE          write process ID to FILE\n",
            "--set=NAME=VALUE, -s NAME=VALUE\n",
            "                        set/override setting (may be repeated)\n",
            "--pdb                   enable pdb on failure\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHjxCzjUxZwd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2BJM0szvgbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}